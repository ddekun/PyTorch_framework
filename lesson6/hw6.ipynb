{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOJpdYBcfOaKfiUfUjwqwe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddekun/PyTorch_framework/blob/lesson6/lesson6/hw6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Фреймворк PyTorch для разработки искусственных нейронных сетей"
      ],
      "metadata": {
        "id": "jgxlXqjKLh3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Урок 6. Нейросети в обработке текста"
      ],
      "metadata": {
        "id": "cLeKxiloLkeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуйте обучить нейронную сеть с применением одномерных сверток для предсказания сентимента сообщений с твитера на примере https://www.kaggle.com/datasets/arkhoshghalb/twitter-sentiment-analysis-hatred-speech\n",
        "Опишите какой результат вы получили? Что помогло вам улучшить ее точность?"
      ],
      "metadata": {
        "id": "YfTVvJVhLmbu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giueV1WsLfKC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "from string import punctuation\n",
        "from stop_words import get_stop_words\n",
        "# from pymorphy2 import MorphAnalyzer\n",
        "\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 1000\n",
        "max_len = 10\n",
        "num_classes = 1\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 512\n",
        "print_batch_n = 100"
      ],
      "metadata": {
        "id": "PRk6t6j-LrwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "# test_df = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "GZdpbwHWLrto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "NgLPMRGxLrqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.label.value_counts()"
      ],
      "metadata": {
        "id": "q5clSYEoLrna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid = train_test_split(train_df, test_size=0.25, random_state=42)\n",
        "X_train.shape, X_valid.shape"
      ],
      "metadata": {
        "id": "3yKKoJSSLrkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.label.value_counts()"
      ],
      "metadata": {
        "id": "a-0vqOCuLrh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid.label.value_counts()"
      ],
      "metadata": {
        "id": "mYPYwZr9Lre6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw = set(get_stop_words(\"en\"))"
      ],
      "metadata": {
        "id": "9GjUFR4lLrb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "puncts = set(punctuation)"
      ],
      "metadata": {
        "id": "yf21YadlLrY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morpher = WordNetLemmatizer()\n",
        "morpher.lemmatize('dogs')"
      ],
      "metadata": {
        "id": "PCkmDHrhLq8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(txt):\n",
        "    txt = str(txt)\n",
        "    txt = \"\".join(c for c in txt if c not in puncts)\n",
        "    txt = txt.lower()\n",
        "#     txt = re.sub(\"не\\s\", \"не\", txt)\n",
        "    txt = [morpher.lemmatize(word) for word in txt.split() if word not in sw]\n",
        "    return \" \".join(txt)"
      ],
      "metadata": {
        "id": "ktXO4cuvL8ta"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "X_train.tweet = X_train.tweet.progress_apply(preprocess_text)\n",
        "X_valid.tweet = X_valid.tweet.progress_apply(preprocess_text)"
      ],
      "metadata": {
        "id": "vjwvybOZL8qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus = \" \".join(X_train.tweet)\n",
        "train_corpus = train_corpus.lower()\n",
        "tokens = word_tokenize(train_corpus)\n",
        "tokens[:5]"
      ],
      "metadata": {
        "id": "TK8YImzDL8nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Отфильтруем данные и соберём в корпус N наиболее частых токенов\n",
        "tokens_filtered = [word for word in tokens if word.isalnum()]"
      ],
      "metadata": {
        "id": "i7_GNgqlL8kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "dist = FreqDist(tokens_filtered)\n",
        "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]  # вычитание 1 для padding\n",
        "len(tokens_filtered_top)"
      ],
      "metadata": {
        "id": "t2r_zlhSL8hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_filtered_top[:10]"
      ],
      "metadata": {
        "id": "U4Glg1q-L8dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}"
      ],
      "metadata": {
        "id": "95Oaa4xtL8ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_sequence(text, maxlen):\n",
        "\n",
        "    result = []\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens_filtered = [word for word in tokens if word.isalnum()] # проверка чтобы токен был либо буквенный либо символьный\n",
        "    for word in tokens_filtered:\n",
        "        if word in vocabulary:\n",
        "            result.append(vocabulary[word])\n",
        "\n",
        "    padding = [0] * (maxlen-len(result))\n",
        "\n",
        "    return result[-maxlen:] + padding"
      ],
      "metadata": {
        "id": "nUGeyOBDMFm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "x_train = np.asarray([text_to_sequence(text, max_len) for text in X_train.tweet])\n",
        "x_val = np.asarray([text_to_sequence(text, max_len) for text in X_valid.tweet])\n",
        "\n",
        "x_train.shape, x_val.shape"
      ],
      "metadata": {
        "id": "dbUC0IsrMFkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.tweet.iloc[0]"
      ],
      "metadata": {
        "id": "B63AKhpGMFho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "id": "zo1UIzd9MFeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size=2000, embedding_dim=64, out_channel=64, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_1 = nn.Conv1d(embedding_dim, out_channel, kernel_size=2)\n",
        "        self.conv_2 = nn.Conv1d(embedding_dim, out_channel, kernel_size=3)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear_1 = nn.Linear(out_channel, out_channel // 2)\n",
        "        self.linear_2 = nn.Linear(out_channel // 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.embedding(x) # B, L, E\n",
        "        #                       B  E  L\n",
        "        output = output.permute(0, 2, 1)\n",
        "        output = self.conv_1(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.pool(output)\n",
        "\n",
        "        output = self.conv_2(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.pool(output)\n",
        "        output = torch.max(output, axis=2).values\n",
        "        output = self.linear_1(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.linear_2(output)\n",
        "        output = F.sigmoid(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "WLi4A5MUMFbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataWrapper(Dataset):\n",
        "    def __init__(self, data, target, transform=None):\n",
        "        self.data = torch.from_numpy(data).long() # если индексы инты то делать лучше лонг - иначе может сломаться вдруг -\n",
        "        # так как на инт меньше памяти выделяется на каждое значение и может переполниться\n",
        "        self.target = torch.from_numpy(target).long() # какие то ф-и потерь на интах не считается - а только на лонгах\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "3SDQ3O_2MFYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DataWrapper(x_train, X_train.label.values)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = DataWrapper(x_val, X_valid.label.values)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "LgEQNDKJMPds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader.sampler"
      ],
      "metadata": {
        "id": "079nXx4fMPaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, l in train_loader:\n",
        "    print(x.shape)\n",
        "    print(l.shape)\n",
        "    print(l[0])\n",
        "    break"
      ],
      "metadata": {
        "id": "6m1UnuwmMQBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(vocab_size=max_words)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "JZ9Mnmn0MP-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "print(\"Parameters:\", sum([param.nelement() for param in model.parameters()]))"
      ],
      "metadata": {
        "id": "PhBhndKRMP7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # засовываем все параметры - они пока необученные\n",
        "criterion = nn.BCELoss() # класс у нас 1 - поэтому бинарная кросс энтропия тут справляется"
      ],
      "metadata": {
        "id": "TZNppTalMP4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "model.train()\n",
        "th = 0.5\n",
        "\n",
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_items, running_right = 0.0, 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # обнуляем градиент\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels.float().view(-1, 1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # подсчет ошибки на обучении\n",
        "        loss = loss.item()\n",
        "        running_items += len(labels)\n",
        "        # подсчет метрики на обучении\n",
        "        pred_labels = torch.squeeze((outputs > th).int())\n",
        "        running_right += (labels == pred_labels).sum()\n",
        "\n",
        "    # выводим статистику о процессе обучения (переводим модель в валидацию)\n",
        "    model.eval()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n",
        "            f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
        "            f'Loss: {loss:.3f}. ' \\\n",
        "            f'Acc: {running_right / running_items:.3f}', end='. ')\n",
        "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
        "    train_loss_history.append(loss)\n",
        "\n",
        "        # выводим статистику на тестовых данных\n",
        "    test_running_right, test_running_total, test_loss = 0.0, 0.0, 0.0\n",
        "    for j, data in enumerate(val_loader):\n",
        "        test_labels = data[1].to(device)\n",
        "        test_outputs = model(data[0].to(device))\n",
        "\n",
        "        # подсчет ошибки на тесте\n",
        "        test_loss = criterion(test_outputs, test_labels.float().view(-1, 1))\n",
        "        # подсчет метрики на тесте\n",
        "        test_running_total += len(data[1])\n",
        "        pred_test_labels = torch.squeeze((test_outputs > th).int())\n",
        "        test_running_right += (test_labels == pred_test_labels).sum()\n",
        "\n",
        "    test_loss_history.append(test_loss.item())\n",
        "    print(f'Test loss: {test_loss:.3f}. Test acc: {test_running_right / test_running_total:.3f}')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "print('Training is finished!')"
      ],
      "metadata": {
        "id": "Got-gfNAMP1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Loss history')\n",
        "plt.grid(True)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.plot(train_loss_history, label='train')\n",
        "plt.plot(test_loss_history, label='test')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "LIMfaHZMMaw4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}